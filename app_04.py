import io
import uuid
import pickle
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import datetime
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, ConversationalRetrievalChain
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os

# åˆå§‹åŒ–ç¯å¢ƒå˜é‡
load_dotenv()
plt.style.use('ggplot')


# --------------------------
# é€šç”¨å·¥å…·å‡½æ•°
# --------------------------
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰ä¼šè¯çŠ¶æ€"""
    if 'current_mode' not in st.session_state:
        st.session_state.current_mode = "ğŸ’¬ æ™ºèƒ½èŠå¤©"

    # èŠå¤©æ¨¡å—çŠ¶æ€
    if 'chat_messages' not in st.session_state:
        st.session_state.chat_messages = [{'role': 'ai', 'content': 'ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ'}]
    if 'chat_memory' not in st.session_state:
        st.session_state.chat_memory = ConversationBufferMemory(return_messages=True)

    # æ–‡æ¡£é—®ç­”æ¨¡å—çŠ¶æ€
    if 'rag_memory' not in st.session_state:
        st.session_state.rag_memory = ConversationBufferMemory(
            return_messages=True,
            memory_key='chat_history',
            output_key='answer'
        )
    if 'session_id' not in st.session_state:
        st.session_state.session_id = uuid.uuid4().hex
    if 'rag_db' not in st.session_state:
        st.session_state.rag_db = None
    if 'is_new_file' not in st.session_state:
        st.session_state.is_new_file = True

    # æ•°æ®åˆ†ææ¨¡å—çŠ¶æ€
    if 'data_memory' not in st.session_state:
        st.session_state.data_memory = ConversationBufferMemory(return_messages=True)
    if 'data_df' not in st.session_state:
        st.session_state.data_df = None


def get_ai_response(memory, user_prompt, system_prompt=""):
    """é€šç”¨AIå“åº”ç”Ÿæˆå‡½æ•°"""
    try:
        model = ChatOpenAI(
            model=st.session_state.selected_model,
            api_key=st.secrets['API_KEY'],
            base_url='https://api.openai.com/v1',
            temperature=st.session_state.model_temperature,
            max_tokens=st.session_state.model_max_length
        )
        chain = ConversationChain(llm=model, memory=memory)
        full_prompt = f"System: {system_prompt}\nUser: {user_prompt}"
        response = chain.invoke({'input': full_prompt})['response']
        return response
    except Exception as e:
        st.error(f"AIè¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥æˆ–APIå¯†é’¥é…ç½®ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
        return "æ— æ³•ç”Ÿæˆå“åº”ï¼Œè¯·æ£€æŸ¥é…ç½®"


# --------------------------
# ä¾§è¾¹æ åŠŸèƒ½
# --------------------------
def render_sidebar():
    with st.sidebar:
        st.title("âš™ï¸ æ§åˆ¶é¢æ¿")

        # æ¨¡å‹é…ç½®
        st.subheader("æ¨¡å‹è®¾ç½®")
        st.session_state.selected_model = st.selectbox(
            "AIæ¨¡å‹",
            ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo", "gpt-4"],
            index=0,
            help="é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹"
        )
        st.session_state.model_temperature = st.slider(
            "æ¸©åº¦å€¼",
            0.0, 1.0, 0.7, 0.1,
            help="æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼ˆ0=ä¿å®ˆï¼Œ1=åˆ›æ–°ï¼‰"
        )
        st.session_state.model_max_length = st.slider(
            "æœ€å¤§é•¿åº¦",
            100, 2000, 1000,
            help="æ§åˆ¶å›ç­”çš„æœ€å¤§é•¿åº¦"
        )
        st.session_state.system_prompt = st.text_area(
            "ç³»ç»Ÿæç¤ºè¯",
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡ç®€æ´å‡†ç¡®åœ°å›ç­”é—®é¢˜",
            height=100
        )

        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

        # æ–‡ä»¶ä¸Šä¼ 
        st.subheader("æ•°æ®ç®¡ç†")
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒCSV/Excel/TXTï¼‰",
            type=["csv", "xlsx", "txt"],
            help="æ ¹æ®å½“å‰æ¨¡å¼è‡ªåŠ¨å¤„ç†æ–‡ä»¶ç±»å‹"
        )
        handle_uploaded_file(uploaded_file)

        # å†å²æ¶ˆæ¯
        st.subheader("å¯¹è¯å†å²")
        render_history()

        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰å†å²"):
            clear_current_history()


def handle_uploaded_file(file):
    if not file:
        return

    try:
        file_type = file.name.split('.')[-1]

        # å¤„ç†æ•°æ®æ–‡ä»¶
        if file_type in ['csv', 'xlsx']:
            df = pd.read_csv(file) if file_type == 'csv' else pd.read_excel(file)
            st.session_state.data_df = df
            st.success("æ•°æ®åŠ è½½æˆåŠŸï¼")
            st.dataframe(df.head(8), use_container_width=True, height=300)

        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        elif file_type == 'txt':
            if st.session_state.current_mode == "ğŸ“š æ–‡æ¡£é—®ç­”":
                st.session_state.is_new_file = True
                with open(f'{st.session_state.session_id}.txt', 'w', encoding='utf-8') as f:
                    f.write(file.read().decode('utf-8'))
                st.success("æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼")
            else:
                st.session_state.txt_content = file.read().decode('utf-8')
                st.text_area("æ–‡æœ¬å†…å®¹", st.session_state.txt_content, height=300)

    except Exception as e:
        st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ï¼š{str(e)}")


def render_history():
    """æ¸²æŸ“å†å²è®°å½•ï¼Œå¹¶åœ¨å¤±è´¥æ—¶æä¾›ç”¨æˆ·å‹å¥½çš„æç¤º"""
    try:
        current_history = []
        if st.session_state.current_mode == "ğŸ’¬ æ™ºèƒ½èŠå¤©":
            # è·å–èŠå¤©æ¨¡å¼ä¸‹çš„å†å²è®°å½•
            current_history = [msg["content"] for msg in st.session_state.chat_messages if msg["role"] == "human"]
        elif st.session_state.current_mode == "ğŸ“š æ–‡æ¡£é—®ç­”":
            # è·å–æ–‡æ¡£é—®ç­”æ¨¡å¼ä¸‹çš„å†å²è®°å½•
            rag_memory = st.session_state.get('rag_memory')
            if rag_memory is not None:
                chat_history = rag_memory.load_memory_variables({}).get('chat_history', [])
                current_history = [msg.content for msg in chat_history if isinstance(msg, HumanMessage)]
            else:
                current_history = []

        if current_history:
            # æ˜¾ç¤ºå†å²è®°å½•é€‰æ‹©æ¡†
            selected = st.selectbox(
                "é€‰æ‹©å†å²è®°å½•",
                options=[msg[:30] + "..." for msg in current_history],
                key="history_select"
            )
            if selected:
                st.write(f"**é€‰ä¸­è®°å½•**: {selected}")
        else:
            st.write("æš‚æ— å†å²è®°å½•")
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶æ˜¾ç¤ºå‹å¥½æç¤º
        st.error(f"åŠ è½½å†å²è®°å½•æ—¶é‡åˆ°é—®é¢˜ï¼š{str(e)}ã€‚è¯·å°è¯•åˆ·æ–°é¡µé¢æˆ–é‡æ–°å¯åŠ¨åº”ç”¨ã€‚")
        st.caption("æç¤ºï¼šå¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·æ£€æŸ¥åº”ç”¨æ—¥å¿—ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚")


def clear_current_history():
    try:
        if st.session_state.current_mode == "ğŸ’¬ æ™ºèƒ½èŠå¤©":
            st.session_state.chat_messages = [{'role': 'ai', 'content': 'ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ'}]
            st.session_state.chat_memory.clear()
        elif st.session_state.current_mode == "ğŸ“š æ–‡æ¡£é—®ç­”":
            st.session_state.rag_memory.clear()
        elif st.session_state.current_mode == "ğŸ“Š æ•°æ®åˆ†æ":
            st.session_state.data_memory.clear()
        st.success("å½“å‰æ¨¡å¼å†å²è®°å½•å·²æ¸…é™¤ï¼")
    except Exception as e:
        st.error(f"æ¸…ç©ºå†å²å¤±è´¥ï¼š{str(e)}")


# --------------------------
# ä¸»åŠŸèƒ½æ¨¡å—
# --------------------------
def main():
    init_session_state()

    # é¡µé¢æ ‡é¢˜
    st.markdown("""
        <div style="text-align:center; margin-bottom:40px">
            <h1 style="margin-bottom:0">SuperAI æ™ºèƒ½åˆ†æåŠ©æ‰‹ğŸš€</h1>
            <p style="color:#6C63FF; font-size:1.2rem">æ•°æ®æ´å¯Ÿä»æœªå¦‚æ­¤ç®€å•</p>
        </div>
    """, unsafe_allow_html=True)

    # æ¸²æŸ“ä¾§è¾¹æ 
    render_sidebar()

    # æ¨¡å¼é€‰æ‹©
    st.session_state.current_mode = st.sidebar.radio(
        "åŠŸèƒ½å¯¼èˆª",
        ["ğŸ’¬ æ™ºèƒ½èŠå¤©", "ğŸ“š æ–‡æ¡£é—®ç­”", "ğŸ“Š æ•°æ®åˆ†æ"],
        horizontal=True,
        label_visibility="collapsed"
    )

    # è·¯ç”±åˆ°å¯¹åº”æ¨¡å—
    if st.session_state.current_mode == "ğŸ’¬ æ™ºèƒ½èŠå¤©":
        render_chat()
    elif st.session_state.current_mode == "ğŸ“š æ–‡æ¡£é—®ç­”":
        render_document_qa()
    elif st.session_state.current_mode == "ğŸ“Š æ•°æ®åˆ†æ":
        render_data_analysis()


def render_chat():
    st.header("ğŸ’¬ æ™ºèƒ½èŠå¤©")

    # æ˜¾ç¤ºæ¶ˆæ¯å†å²
    for msg in st.session_state.chat_messages:
        role = "user" if msg["role"] == "human" else "assistant"
        with st.chat_message(role):
            st.write(msg["content"])

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        handle_chat_input(prompt)


def handle_chat_input(prompt):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.chat_messages.append({'role': 'human', 'content': prompt})

    # è·å–AIå“åº”
    with st.spinner('æ€è€ƒä¸­...'):
        response = get_ai_response(
            memory=st.session_state.chat_memory,
            user_prompt=prompt,
            system_prompt=st.session_state.system_prompt
        )

    # æ·»åŠ AIå“åº”
    st.session_state.chat_messages.append({'role': 'ai', 'content': response})
    st.rerun()


def render_document_qa():
    st.header("ğŸ“š æ–‡æ¡£æ™ºèƒ½é—®ç­”")

    # é—®é¢˜è¾“å…¥
    question = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", key="doc_question")

    if question and st.session_state.get('session_id'):
        with st.spinner('æ­£åœ¨åˆ†ææ–‡æ¡£...'):
            try:
                if st.session_state.is_new_file:
                    loader = TextLoader(f'{st.session_state.session_id}.txt', encoding='utf-8')
                    docs = loader.load()

                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
                    )
                    texts = text_splitter.split_documents(docs)

                    st.session_state.rag_db = FAISS.from_documents(
                        texts,
                        embedding=None  # ä½¿ç”¨é»˜è®¤çš„embedding
                    )
                    st.session_state.is_new_file = False

                model = ChatOpenAI(
                    model=st.session_state.selected_model,
                    api_key=st.secrets['API_KEY'],
                    base_url='https://api.openai.com/v1',
                    temperature=0.2
                )

                qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=model,
                    retriever=st.session_state.rag_db.as_retriever(
                        search_type="mmr",
                        search_kwargs={'k': 3}
                    ),
                    memory=st.session_state.rag_memory,
                    return_source_documents=True
                )

                result = qa_chain({'question': question})

                st.subheader("ç­”æ¡ˆ")
                st.write(result['answer'])

                with st.expander("å‚è€ƒæ–‡æ¡£ç‰‡æ®µ"):
                    for doc in result['source_documents']:
                        st.markdown(f"**æ¥æº**: `{doc.metadata['source']}`")
                        st.text(doc.page_content[:300] + "...")
                        st.divider()

            except Exception as e:
                st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥ï¼š{str(e)}")


def render_data_analysis():
    st.header("ğŸ“Š æ™ºèƒ½æ•°æ®åˆ†æ")

    if st.session_state.data_df is not None:
        st.write("### æ•°æ®é¢„è§ˆ")
        st.dataframe(st.session_state.data_df.head(10), use_container_width=True)

        analysis_query = st.text_input("è¾“å…¥åˆ†æéœ€æ±‚ï¼ˆç¤ºä¾‹ï¼šæ˜¾ç¤ºå„æœˆé”€å”®é¢è¶‹åŠ¿ï¼‰")

        if analysis_query:
            with st.spinner('æ­£åœ¨ç”Ÿæˆåˆ†æ...'):
                try:
                    data_desc = f"""
                    æ•°æ®é›†åˆ—åï¼š{st.session_state.data_df.columns.tolist()}
                    æ•°æ®ç±»å‹ï¼š
                    {st.session_state.data_df.dtypes.to_string()}
                    æ•°æ®æ ·ä¾‹ï¼š
                    {st.session_state.data_df.head(3).to_markdown()}
                    """

                    analysis_result = get_ai_response(
                        memory=st.session_state.data_memory,
                        user_prompt=f"åˆ†æéœ€æ±‚ï¼š{analysis_query}\n{data_desc}",
                        system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„æ•°æ®é›†è¿›è¡Œä¸“ä¸šåˆ†æï¼Œç»™å‡ºå¯è§†åŒ–å»ºè®®"
                    )

                    st.write("### AIåˆ†ææŠ¥å‘Š")
                    st.write(analysis_result)

                    # è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨
                    if "æŸ±çŠ¶å›¾" in analysis_result:
                        st.bar_chart(st.session_state.data_df.select_dtypes(include='number'))
                    elif "æŠ˜çº¿å›¾" in analysis_result:
                        st.line_chart(st.session_state.data_df.select_dtypes(include='number'))
                    elif "æ•£ç‚¹å›¾" in analysis_result:
                        st.scatter_chart(st.session_state.data_df)

                except Exception as e:
                    st.error(f"åˆ†æå¤±è´¥ï¼š{str(e)}")
    else:
        st.info("è¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ•°æ®æ–‡ä»¶")


if __name__ == "__main__":
    main()